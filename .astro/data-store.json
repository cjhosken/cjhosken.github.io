[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.3.1","content-config-digest","1acac442e7cd5450","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://cjhosken.github.io\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"serializeConfig\":false},\"legacy\":{\"collections\":false}}","posts",["Map",11,12,40,41,64,65,89,90,128,129,157,158,179,180,198,199,216,217],"breeze",{"id":11,"data":13,"body":20,"filePath":21,"digest":22,"rendered":23,"legacyId":39},{"title":14,"date":15,"role":16,"img":17,"excerpt":18,"type":19},"Breeze","2022-08-13","Solo Project","breeze_render_engine.png","A 3D application with a simple built-in raytracer written in C++ with Qt and OpenGL.","other","![Cover](/images/content/breeze/breeze_render_engine.png)\n\n## Overview\n\nBreeze Render Engine is a C++ application that allows artists to create scenes in OpenGL and then render them out with Breeze's custom render engine.\n\nBuilt off the Ray Tracing in One Weekend tutorial series, Breeze Render Engine was written with C++, OpenGL, and Qt.\n\nA screenshot of a scene made in Breeze. You can download and try the application from https://github.com/cjhosken/breeze_render_engine. \n\n## Usage\n\nA screenshot of the viewport in use. I wanted to especially focus on the professional look of the application and tried to make it as minimal as possible.\n\n![Objects](/images/content/breeze/objs.png)\n\nThe application could also read in .obj files for rendering.\n\n## Tutorial\n\nA brief tutorial was made showing the application in action.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/breeze/breeze_demo.mp4\" type=\"video/mp4\">\n\u003C/video>\n\nThe image that was rendered in the breeze demo video.\n\n![Render](/images/content/breeze/render.png)\n\n*Written August 13, 2022 by Christopher Hosken*","src/content/posts/breeze.md","cc541e2523998077",{"html":24,"metadata":25},"\u003Cp>\u003Cimg src=\"/images/content/breeze/breeze_render_engine.png\" alt=\"Cover\">\u003C/p>\n\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>Breeze Render Engine is a C++ application that allows artists to create scenes in OpenGL and then render them out with Breeze’s custom render engine.\u003C/p>\n\u003Cp>Built off the Ray Tracing in One Weekend tutorial series, Breeze Render Engine was written with C++, OpenGL, and Qt.\u003C/p>\n\u003Cp>A screenshot of a scene made in Breeze. You can download and try the application from \u003Ca href=\"https://github.com/cjhosken/breeze_render_engine\">https://github.com/cjhosken/breeze_render_engine\u003C/a>.\u003C/p>\n\u003Ch2 id=\"usage\">Usage\u003C/h2>\n\u003Cp>A screenshot of the viewport in use. I wanted to especially focus on the professional look of the application and tried to make it as minimal as possible.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/breeze/objs.png\" alt=\"Objects\">\u003C/p>\n\u003Cp>The application could also read in .obj files for rendering.\u003C/p>\n\u003Ch2 id=\"tutorial\">Tutorial\u003C/h2>\n\u003Cp>A brief tutorial was made showing the application in action.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/breeze/breeze_demo.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Cp>The image that was rendered in the breeze demo video.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/breeze/render.png\" alt=\"Render\">\u003C/p>\n\u003Cp>\u003Cem>Written August 13, 2022 by Christopher Hosken\u003C/em>\u003C/p>",{"headings":26,"imagePaths":37,"frontmatter":38},[27,31,34],{"depth":28,"slug":29,"text":30},2,"overview","Overview",{"depth":28,"slug":32,"text":33},"usage","Usage",{"depth":28,"slug":35,"text":36},"tutorial","Tutorial",[],{"title":14,"type":19,"date":15,"role":16,"excerpt":18,"img":17},"breeze.md","hdtemplate",{"id":40,"data":42,"body":47,"filePath":48,"digest":49,"rendered":50,"legacyId":63},{"title":43,"date":44,"role":16,"img":45,"excerpt":46,"type":19},"HdTemplate","2025-01-01","cover.png","A simple Hydra Delegate raytracer written for UsdView and Houdini Solaris.","![UsdView](/images/content/hdtemplate/cover.png)\n\n## Overview\n\nIn order to understand how USD is making moves in the world of software development I decided to experiment with making a Hydra Delegate raytracer. The delegate works in multiple DCCs and the Github repository can be found at https://github.com/cjhosken/hdTemplateRenderer.\n\n## UsdView\n\nThe delegate was originally built for UsdView in C++. Using a BVH structure and a hard-coded distant light, the USD Kitchen Scene could be rendered. \n\nThe render shown below is the HdTemplateRenderer after ~ 10 minutes. The basic sphere scene was rendered in ~ 30 seconds.\n\n![UsdView](/images/content/hdtemplate/cover.png)\n![UsdView](/images/content/hdtemplate/simple.png)\n\nA render of the USD Kitchen Scene in Usdview using HdTemplateRenderer after ~ 10 minutes.\n\n## Houdini\n\nUsing the Houdini USD build, the delegate was expanded for use in Houdini Solaris. The delegate has 3 AOVs, Beauty, Normal, Depth.\n\n![Houdini Beauty](/images/content/hdtemplate/hou_beauty.png)\n![Houdini Depth](/images/content/hdtemplate/hou_depth.png)\n![Houdini Normal](/images/content/hdtemplate/hou_normal.png)\n\n*Written Jan 1, 2025 by Christopher Hosken*","src/content/posts/hdtemplate.md","1a80f5593414789e",{"html":51,"metadata":52},"\u003Cp>\u003Cimg src=\"/images/content/hdtemplate/cover.png\" alt=\"UsdView\">\u003C/p>\n\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>In order to understand how USD is making moves in the world of software development I decided to experiment with making a Hydra Delegate raytracer. The delegate works in multiple DCCs and the Github repository can be found at \u003Ca href=\"https://github.com/cjhosken/hdTemplateRenderer\">https://github.com/cjhosken/hdTemplateRenderer\u003C/a>.\u003C/p>\n\u003Ch2 id=\"usdview\">UsdView\u003C/h2>\n\u003Cp>The delegate was originally built for UsdView in C++. Using a BVH structure and a hard-coded distant light, the USD Kitchen Scene could be rendered.\u003C/p>\n\u003Cp>The render shown below is the HdTemplateRenderer after ~ 10 minutes. The basic sphere scene was rendered in ~ 30 seconds.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/hdtemplate/cover.png\" alt=\"UsdView\">\n\u003Cimg src=\"/images/content/hdtemplate/simple.png\" alt=\"UsdView\">\u003C/p>\n\u003Cp>A render of the USD Kitchen Scene in Usdview using HdTemplateRenderer after ~ 10 minutes.\u003C/p>\n\u003Ch2 id=\"houdini\">Houdini\u003C/h2>\n\u003Cp>Using the Houdini USD build, the delegate was expanded for use in Houdini Solaris. The delegate has 3 AOVs, Beauty, Normal, Depth.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/hdtemplate/hou_beauty.png\" alt=\"Houdini Beauty\">\n\u003Cimg src=\"/images/content/hdtemplate/hou_depth.png\" alt=\"Houdini Depth\">\n\u003Cimg src=\"/images/content/hdtemplate/hou_normal.png\" alt=\"Houdini Normal\">\u003C/p>\n\u003Cp>\u003Cem>Written Jan 1, 2025 by Christopher Hosken\u003C/em>\u003C/p>",{"headings":53,"imagePaths":61,"frontmatter":62},[54,55,58],{"depth":28,"slug":29,"text":30},{"depth":28,"slug":56,"text":57},"usdview","UsdView",{"depth":28,"slug":59,"text":60},"houdini","Houdini",[],{"title":43,"date":44,"role":16,"excerpt":46,"img":45,"type":19},"hdtemplate.md","fif",{"id":64,"data":66,"body":73,"filePath":74,"digest":75,"rendered":76,"legacyId":88},{"title":67,"date":68,"role":69,"img":70,"excerpt":71,"type":72},"Fruits in the Fridge","2024-07-08","Team Project","Cover.png","A couple of fruits doing what they love most; Dancing!","project","![Cover](/images/content/fif/Cover.png)\n\n\n## Introduction\n\nFruits in the Fridge was a change for me and Gwendoline Eftimakis to understand how to use Maya in a full animation environment. We were inspired by sensory animations for kids and wante to make something cute and lighthearted.\n\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/L4Xab8dQxDw?si=-zo3Bhxo7fXAssqD\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen>\u003C/iframe>\n\n## Breakdown\n\nThe animation for Fruits in the Fridge was done inside of Maya.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/fif/viewport.mp4\" type=\"video/mp4\">\n\u003C/video>","src/content/posts/fif.md","87ef85c896f0e3b6",{"html":77,"metadata":78},"\u003Cp>\u003Cimg src=\"/images/content/fif/Cover.png\" alt=\"Cover\">\u003C/p>\n\u003Ch2 id=\"introduction\">Introduction\u003C/h2>\n\u003Cp>Fruits in the Fridge was a change for me and Gwendoline Eftimakis to understand how to use Maya in a full animation environment. We were inspired by sensory animations for kids and wante to make something cute and lighthearted.\u003C/p>\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/L4Xab8dQxDw?si=-zo3Bhxo7fXAssqD\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen>\u003C/iframe>\n\u003Ch2 id=\"breakdown\">Breakdown\u003C/h2>\n\u003Cp>The animation for Fruits in the Fridge was done inside of Maya.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/fif/viewport.mp4\" type=\"video/mp4\">\n\u003C/video>",{"headings":79,"imagePaths":86,"frontmatter":87},[80,83],{"depth":28,"slug":81,"text":82},"introduction","Introduction",{"depth":28,"slug":84,"text":85},"breakdown","Breakdown",[],{"title":67,"date":68,"role":69,"excerpt":71,"img":70,"type":72},"fif.md","creator",{"id":89,"data":91,"body":96,"filePath":97,"digest":98,"rendered":99,"legacyId":127},{"title":92,"date":93,"role":16,"img":94,"excerpt":95,"type":72},"The Creator Case Study","2023-12-14","cover.jpg","A case study on CG head replacements, inspired by an Industrial Light and Magic talk on The Creator.","# Exploring VFX: CG Head Replacement\n\nInspired by ILM's latest work on The Creator, The National Centre for Computer Animation – Bournemouth University student, Christopher Hosken, embarked on the challenge of exdcuting a CG head replacement, driven by the creative spark ignited by their cutting-edge techniques. In this article, Christopher takes a deeper dive into his processes, challenges and successes with this project.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/creator/TheCreatorStudy_mp4.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n## About The Project\nEvery year, Bournemouth University hosts the [BFX Festival]() in which artists and studios come to give talks and connect with students. One of the talks was from [*Industrial Light and Magic*]() on [*The Creator*](), and as I had recently seen *The Creator* and was blown away by the visuals, I decided to attend.\n\nThe talk completely shifted my views on VFX. The way ILM described their work made it sound like it was a walk in the park, especially the CG head replacements, which the film has a LOT of. I came out of the talk thinking, *\"If ILM makes it sound so easy, why don't I try a CG head replacement shot myself?\"*\n\nI specifically wanted to try and integrate a robot head into a shot without any other assets. No information about the set, No on-set HDRIs, just the clip. While it may not be true, I believed that for a film like *The Creator*, this is what most of the artists were dealing with.\n\n\n## Plate Preparation\n\nI started by finding footage on Action VFX that would match the same theme as *The Creator*. Since the film is about a war between humanity and AI, I wanted something that felt war-related and sci-fi.\n\nI ended up choosing a short clip of a solider being punched by a guard. I liked the shot because I could showcase the CG head integration well. I also decided to integrate a CG hand because I thought *\"If I'm going to do this, why not go the whole way.\"*.\n\n![Raw](/images/content/creator/raw_still.jpg) \u003Cbr>\n\u003Csmall> (Footage from ActionVFX) \u003C/small>\n\nOnce I had my clip, I began tracking the shot and rotoscoping individual elements. Due to the defocused background, I had to grade and sharpen the footage to get a stable track. Once I had the track, I started to reconstruct the backplate.\n\nThe backplate took the most amount of time in this project. It was especially difficult to build behind the subjects head as there wasn't much data. I ended up doing a combination of roto-masks, roto-paints, and 3D cards. The reason for 3D cards was because of how the actor's arm goes across the entire shot. It would mess up any mask I made, and since I had a pretty good track, I used cards to re-build the back walls.\n\nAlthough some regions of the backplate weren’t completely accurate, I knew that the robot head and the general action of the scene would distract the audience and stop them from noticing.\n\n![Raw](/images/content/creator/cleanplate_still.jpg)\n\nI then used [KeenTools]() to track the actors head. I had never used KeenTools before, but I heard ILM mention it multiple times in their talk. It took a couple attempts to get a decent track, but once I started to figure it out I understood why ILM used it. Some frames needed to be manually tracked, but it was a lot more efficient than doing manual point tracks.\n\nOnce complete, I exported the head track to [Blender]() and began working on the 3D elements.\n\n## 3D Elements\n\nFor modelling, I built the head and hand using [Adrian Rutkowski’s Kitbashing library](). Asset creation wasn't the primary focus of this project, so I wanted to design the models as quickly as possible. I started with a base mesh and then iterated on top of it by duplicating, scaling, and editing other kit bash elements.\n\nFor texturing, I used the premade [Substance 3D]() materials and changed the colours of different parts. I also added small decals to make the character a bit more interesting. The colours I chose were supposed to be similar to that of police colours. I felt that making the robot seem like it was part of the police would kick off the audience's imagination at to what might be happening.\n\n\n![Hand](/images/content/creator/hand_turntable.jpg) ![Head](/images/content/creator/head_turntable.jpg)\n\nOnce I finished the models, I threw them into a lookdev environment to see how they would look under realistic lighting. I wasn't too worried about the brightness and saturation of the colours as I planned on colour matching them in [Nuke](), I just wanted to make sure that materials were reflecting as they should.\n\nI then the KeenTools track and attached the robot head to it. I wasn't able to figure out how to export my KeenTools head with correct scale, so I had to a bit of constraint magic to place the CG head at the right distance. I also had to do a bit of repositioning in some frames so that the track looked more consistent.\n\nFor the hand, I track it by eye into the shot, and then animated it to close into a fist. I did this by quickly rigging the hand with Rigify and then animating it with animation controls. The hardest part with animation was getting it to match the footage correctly, there were moments where the head or hand would slide around in place, and dialing that in took a lot of time.\n\n![Hand](/images/content/creator/viewport.jpg)\u003Cbr>\n\u003Csmall>(I also used a jacket model from csheffield on Sketchfab to cast shadows where the neck would meet the jacket in the shot.)\u003C/small>\n\nOnce animation was complete, I lit the scene, rendered it out, and then dived back into Nuke.\n\n## Integration\n\nIntegrating the 3D elements into the shot also took some time. In the rendering stage I create different light passes that I could combine in comp. The different passes made it a lot easier for me to match the lighting to the shot. Without it, I would have had to re-render the scene multiple times.\n\nFor the grade, I was lucky enough to attend Victor Perez's compositing masterclass at the BFX Festival. The class taught me the basics of the Nuke grade node and how to integrate CG elements into shots. Without learning from him, I would have struggled a lot more on matching the grade between the elements.\n\nOnce the grain and chromatic aberration were added, I exported the shot into DaVinci Resolve and added a cinematic grade with SFX and music.\n\n![Hand](/images/content/creator/nuke_script.jpg) \n\n## Conclusion\n\nOverall I’m quite happy with the final shot. I was able to learn a lot about compositing, especially with doing backplate reconstruction and using KeenTools.\n\nIf I was to continue working on the shot, I would probably focus more on my lighting as well as refine my roto-masks. There a couple area's of the shot in which shadows aren't realistic and parts of the backplate morph and disappear. However, I feel that for a quick study on The Creator, it looks great.\n\nYou can watch the full breakdown below:\n\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BRdFg-8wEYE?si=AB6Ppzpw4KFl1f03\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen>\u003C/iframe>\n\n## Lessons Learned\n\nMore recently I've discovered that I learn a lot more doing the long projects than the short ones, and this project was no exception. I am used to working on projects and then move onto something new as soon as I get stuck, but I've found that pushing through the discomfort of things not working has made me a much better artist over the past couple of months.\n\nIn terms of actual technical knowledge, I've found that doing everything step-by-step is whole lot easier than trying to do it in parallel. Don't try and integrate your CG while you're still doing your backplate. Make each part look perfect on it's own, and then bring it all together at the end.\n\nI'm definitely going to use KeenTools in my future projects, I found that not only does it do heads, but anything! It's a very cool tool and I'm excited to use it more.\n\n*Written Feb 6, 2024 by Christopher Hosken*","src/content/posts/creator.md","8f7cdda460237608",{"html":100,"metadata":101},"\u003Ch1 id=\"exploring-vfx-cg-head-replacement\">Exploring VFX: CG Head Replacement\u003C/h1>\n\u003Cp>Inspired by ILM’s latest work on The Creator, The National Centre for Computer Animation – Bournemouth University student, Christopher Hosken, embarked on the challenge of exdcuting a CG head replacement, driven by the creative spark ignited by their cutting-edge techniques. In this article, Christopher takes a deeper dive into his processes, challenges and successes with this project.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/creator/TheCreatorStudy_mp4.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Ch2 id=\"about-the-project\">About The Project\u003C/h2>\n\u003Cp>Every year, Bournemouth University hosts the \u003Ca href=\"\">BFX Festival\u003C/a> in which artists and studios come to give talks and connect with students. One of the talks was from \u003Ca href=\"\">\u003Cem>Industrial Light and Magic\u003C/em>\u003C/a> on \u003Ca href=\"\">\u003Cem>The Creator\u003C/em>\u003C/a>, and as I had recently seen \u003Cem>The Creator\u003C/em> and was blown away by the visuals, I decided to attend.\u003C/p>\n\u003Cp>The talk completely shifted my views on VFX. The way ILM described their work made it sound like it was a walk in the park, especially the CG head replacements, which the film has a LOT of. I came out of the talk thinking, \u003Cem>“If ILM makes it sound so easy, why don’t I try a CG head replacement shot myself?”\u003C/em>\u003C/p>\n\u003Cp>I specifically wanted to try and integrate a robot head into a shot without any other assets. No information about the set, No on-set HDRIs, just the clip. While it may not be true, I believed that for a film like \u003Cem>The Creator\u003C/em>, this is what most of the artists were dealing with.\u003C/p>\n\u003Ch2 id=\"plate-preparation\">Plate Preparation\u003C/h2>\n\u003Cp>I started by finding footage on Action VFX that would match the same theme as \u003Cem>The Creator\u003C/em>. Since the film is about a war between humanity and AI, I wanted something that felt war-related and sci-fi.\u003C/p>\n\u003Cp>I ended up choosing a short clip of a solider being punched by a guard. I liked the shot because I could showcase the CG head integration well. I also decided to integrate a CG hand because I thought \u003Cem>“If I’m going to do this, why not go the whole way.”\u003C/em>.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/creator/raw_still.jpg\" alt=\"Raw\"> \u003Cbr>\n\u003Csmall> (Footage from ActionVFX) \u003C/small>\u003C/p>\n\u003Cp>Once I had my clip, I began tracking the shot and rotoscoping individual elements. Due to the defocused background, I had to grade and sharpen the footage to get a stable track. Once I had the track, I started to reconstruct the backplate.\u003C/p>\n\u003Cp>The backplate took the most amount of time in this project. It was especially difficult to build behind the subjects head as there wasn’t much data. I ended up doing a combination of roto-masks, roto-paints, and 3D cards. The reason for 3D cards was because of how the actor’s arm goes across the entire shot. It would mess up any mask I made, and since I had a pretty good track, I used cards to re-build the back walls.\u003C/p>\n\u003Cp>Although some regions of the backplate weren’t completely accurate, I knew that the robot head and the general action of the scene would distract the audience and stop them from noticing.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/creator/cleanplate_still.jpg\" alt=\"Raw\">\u003C/p>\n\u003Cp>I then used \u003Ca href=\"\">KeenTools\u003C/a> to track the actors head. I had never used KeenTools before, but I heard ILM mention it multiple times in their talk. It took a couple attempts to get a decent track, but once I started to figure it out I understood why ILM used it. Some frames needed to be manually tracked, but it was a lot more efficient than doing manual point tracks.\u003C/p>\n\u003Cp>Once complete, I exported the head track to \u003Ca href=\"\">Blender\u003C/a> and began working on the 3D elements.\u003C/p>\n\u003Ch2 id=\"3d-elements\">3D Elements\u003C/h2>\n\u003Cp>For modelling, I built the head and hand using \u003Ca href=\"\">Adrian Rutkowski’s Kitbashing library\u003C/a>. Asset creation wasn’t the primary focus of this project, so I wanted to design the models as quickly as possible. I started with a base mesh and then iterated on top of it by duplicating, scaling, and editing other kit bash elements.\u003C/p>\n\u003Cp>For texturing, I used the premade \u003Ca href=\"\">Substance 3D\u003C/a> materials and changed the colours of different parts. I also added small decals to make the character a bit more interesting. The colours I chose were supposed to be similar to that of police colours. I felt that making the robot seem like it was part of the police would kick off the audience’s imagination at to what might be happening.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/creator/hand_turntable.jpg\" alt=\"Hand\"> \u003Cimg src=\"/images/content/creator/head_turntable.jpg\" alt=\"Head\">\u003C/p>\n\u003Cp>Once I finished the models, I threw them into a lookdev environment to see how they would look under realistic lighting. I wasn’t too worried about the brightness and saturation of the colours as I planned on colour matching them in \u003Ca href=\"\">Nuke\u003C/a>, I just wanted to make sure that materials were reflecting as they should.\u003C/p>\n\u003Cp>I then the KeenTools track and attached the robot head to it. I wasn’t able to figure out how to export my KeenTools head with correct scale, so I had to a bit of constraint magic to place the CG head at the right distance. I also had to do a bit of repositioning in some frames so that the track looked more consistent.\u003C/p>\n\u003Cp>For the hand, I track it by eye into the shot, and then animated it to close into a fist. I did this by quickly rigging the hand with Rigify and then animating it with animation controls. The hardest part with animation was getting it to match the footage correctly, there were moments where the head or hand would slide around in place, and dialing that in took a lot of time.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/creator/viewport.jpg\" alt=\"Hand\">\u003Cbr>\n\u003Csmall>(I also used a jacket model from csheffield on Sketchfab to cast shadows where the neck would meet the jacket in the shot.)\u003C/small>\u003C/p>\n\u003Cp>Once animation was complete, I lit the scene, rendered it out, and then dived back into Nuke.\u003C/p>\n\u003Ch2 id=\"integration\">Integration\u003C/h2>\n\u003Cp>Integrating the 3D elements into the shot also took some time. In the rendering stage I create different light passes that I could combine in comp. The different passes made it a lot easier for me to match the lighting to the shot. Without it, I would have had to re-render the scene multiple times.\u003C/p>\n\u003Cp>For the grade, I was lucky enough to attend Victor Perez’s compositing masterclass at the BFX Festival. The class taught me the basics of the Nuke grade node and how to integrate CG elements into shots. Without learning from him, I would have struggled a lot more on matching the grade between the elements.\u003C/p>\n\u003Cp>Once the grain and chromatic aberration were added, I exported the shot into DaVinci Resolve and added a cinematic grade with SFX and music.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/creator/nuke_script.jpg\" alt=\"Hand\">\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Overall I’m quite happy with the final shot. I was able to learn a lot about compositing, especially with doing backplate reconstruction and using KeenTools.\u003C/p>\n\u003Cp>If I was to continue working on the shot, I would probably focus more on my lighting as well as refine my roto-masks. There a couple area’s of the shot in which shadows aren’t realistic and parts of the backplate morph and disappear. However, I feel that for a quick study on The Creator, it looks great.\u003C/p>\n\u003Cp>You can watch the full breakdown below:\u003C/p>\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BRdFg-8wEYE?si=AB6Ppzpw4KFl1f03\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen>\u003C/iframe>\n\u003Ch2 id=\"lessons-learned\">Lessons Learned\u003C/h2>\n\u003Cp>More recently I’ve discovered that I learn a lot more doing the long projects than the short ones, and this project was no exception. I am used to working on projects and then move onto something new as soon as I get stuck, but I’ve found that pushing through the discomfort of things not working has made me a much better artist over the past couple of months.\u003C/p>\n\u003Cp>In terms of actual technical knowledge, I’ve found that doing everything step-by-step is whole lot easier than trying to do it in parallel. Don’t try and integrate your CG while you’re still doing your backplate. Make each part look perfect on it’s own, and then bring it all together at the end.\u003C/p>\n\u003Cp>I’m definitely going to use KeenTools in my future projects, I found that not only does it do heads, but anything! It’s a very cool tool and I’m excited to use it more.\u003C/p>\n\u003Cp>\u003Cem>Written Feb 6, 2024 by Christopher Hosken\u003C/em>\u003C/p>",{"headings":102,"imagePaths":125,"frontmatter":126},[103,107,110,113,116,119,122],{"depth":104,"slug":105,"text":106},1,"exploring-vfx-cg-head-replacement","Exploring VFX: CG Head Replacement",{"depth":28,"slug":108,"text":109},"about-the-project","About The Project",{"depth":28,"slug":111,"text":112},"plate-preparation","Plate Preparation",{"depth":28,"slug":114,"text":115},"3d-elements","3D Elements",{"depth":28,"slug":117,"text":118},"integration","Integration",{"depth":28,"slug":120,"text":121},"conclusion","Conclusion",{"depth":28,"slug":123,"text":124},"lessons-learned","Lessons Learned",[],{"title":92,"date":93,"role":16,"excerpt":95,"img":94,"type":72},"creator.md","heman",{"id":128,"data":130,"body":135,"filePath":136,"digest":137,"rendered":138,"legacyId":156},{"title":131,"date":132,"role":133,"img":45,"excerpt":134,"type":72},"Heman","2025-01-17","University Assignment","A stylzed hero asset made in ZBrush, Maya, and Houdini.","![Cover](/images/content/heman/cover.png)\n\n## Modelling\n\nAs part of our Modelling and Tetxturing Assignment at Bournemouth University, we were tasked with making a stylzed hero asset based on a given concept. I decided to choose the Heman concept by Michael McCabe.\n\n![Original Reference](/images/content/heman/heman_doodle.jpg)\n\nWe began by blocking out our character in ZBrush; starting the basic shapes, then the muscles, then the finer details. The hardsurface assets were then modelling in Maya.\n\n![ZBrush](/images/content/heman/heman_zbrush.png)\n\n## Rigging\n\nThe rig was created from scratch in Maya. I hadn't done FK/IK switching before and wanted to give that a go.\n\n![Rigging](/images/content/heman/heman_rig.png)\n\n\n## Houdini & Lookdev\nThe asset was then textured in Substance Painter and brought into Houdini for groom. Heman was rendered in a custom Houdini Lookdev environment that I designed.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/heman/heman_turntable.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/heman/heman_wireframe_turntable.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n\n## USD Pipeline\n\nContinuing on from the [Jupiter Steam Train](/portfolio/jupiter) I built Heman as a USD asset. This is following a new USD Pipeline workflow that I have developed for future projects.\n\n![USD Pipe](/images/content/heman/usd_pipe.png)\n\nThe pipeline worked extremely well for completing a full rounded-character.\n\n\n*Written Jan 17, 2025 by Christopher Hosken*","src/content/posts/heman.md","1520d7fe0869701b",{"html":139,"metadata":140},"\u003Cp>\u003Cimg src=\"/images/content/heman/cover.png\" alt=\"Cover\">\u003C/p>\n\u003Ch2 id=\"modelling\">Modelling\u003C/h2>\n\u003Cp>As part of our Modelling and Tetxturing Assignment at Bournemouth University, we were tasked with making a stylzed hero asset based on a given concept. I decided to choose the Heman concept by Michael McCabe.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/heman/heman_doodle.jpg\" alt=\"Original Reference\">\u003C/p>\n\u003Cp>We began by blocking out our character in ZBrush; starting the basic shapes, then the muscles, then the finer details. The hardsurface assets were then modelling in Maya.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/heman/heman_zbrush.png\" alt=\"ZBrush\">\u003C/p>\n\u003Ch2 id=\"rigging\">Rigging\u003C/h2>\n\u003Cp>The rig was created from scratch in Maya. I hadn’t done FK/IK switching before and wanted to give that a go.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/heman/heman_rig.png\" alt=\"Rigging\">\u003C/p>\n\u003Ch2 id=\"houdini--lookdev\">Houdini &#x26; Lookdev\u003C/h2>\n\u003Cp>The asset was then textured in Substance Painter and brought into Houdini for groom. Heman was rendered in a custom Houdini Lookdev environment that I designed.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/heman/heman_turntable.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/heman/heman_wireframe_turntable.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Ch2 id=\"usd-pipeline\">USD Pipeline\u003C/h2>\n\u003Cp>Continuing on from the \u003Ca href=\"/portfolio/jupiter\">Jupiter Steam Train\u003C/a> I built Heman as a USD asset. This is following a new USD Pipeline workflow that I have developed for future projects.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/heman/usd_pipe.png\" alt=\"USD Pipe\">\u003C/p>\n\u003Cp>The pipeline worked extremely well for completing a full rounded-character.\u003C/p>\n\u003Cp>\u003Cem>Written Jan 17, 2025 by Christopher Hosken\u003C/em>\u003C/p>",{"headings":141,"imagePaths":154,"frontmatter":155},[142,145,148,151],{"depth":28,"slug":143,"text":144},"modelling","Modelling",{"depth":28,"slug":146,"text":147},"rigging","Rigging",{"depth":28,"slug":149,"text":150},"houdini--lookdev","Houdini & Lookdev",{"depth":28,"slug":152,"text":153},"usd-pipeline","USD Pipeline",[],{"title":131,"date":132,"role":133,"excerpt":134,"img":45,"type":72},"heman.md","jupiter",{"id":157,"data":159,"body":163,"filePath":164,"digest":165,"rendered":166,"legacyId":178},{"title":160,"date":161,"role":16,"img":94,"excerpt":162,"type":72},"Jupiter","2024-10-21","A case study on USD asset creation.","![References](/images/content/jupiter/cover.jpg)\n\n## Introduction\n\nUSD has become the \"Buzz Word\" for the animation and visual effects industry. And I felt (as an aspiring Generalist TD), that learning how USD works would be crucial for landing a job outside of university. \n\nIn an effort to explore the capabilities of USD, I decided to create Jupiter, an American steam train from the late 1800s. I've always wanted to model a steam train, and I love the wild-west style of the Jupiter. I spent a total of 2 and half months working on this project, and this is by far my favourite asset I've worked on yet.\n\n![References](/images/content/jupiter/references.png)\n\nI began the project by looking for references online, I used PureRef to bundle them together so I could easily look at them when needed.\n\n## Modelling\n\nTo explore the benefits of USD's cross DCC abilities, I decided to model inside of Blender. I'm a huge fan of Blender's modelling tools, and once I transferred the asset into Maya, I polished the model and created the UVs. This was my \"payload\" file for my USD asset. It contained all the geometry data, including UVs for shading.\n\n![Modelling](/images/content/jupiter/modelling.png)\n\n![UVs](/images/content/jupiter/uvs.png)\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/jupiter/wireframe_turn_for_web.mp4\" type=\"video/mp4\">\n\u003C/video>\n\nAbove is the final model of Jupiter. The hardest part of the asset was getting all the intricate details inside the train, as there weren't many references images of those areas.\n\n\n## Lookdev\n\nOnce the payload was finished, I began working on shading and lookdev. I was able to convert Albin Merle's free Blender lookdev environment into USD, which I could open and use inside of Maya and Houdini.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/jupiter/vibrant_turntable.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n\nWith Substance painter, I working on getting the nice wear and tear of an old train. I was also able to make a shading variant look called \"dull\", which looks cleaner and more toy-like.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/jupiter/dull_turntable.mp4\" type=\"video/mp4\">\n\u003C/video>\n\nI did my rendering inside of Maya using Arnold, however the asset also works with Houdini's Karma.\n\n## Conclusion\n\nOverall, I'm very happy with this asset. If I had more time to work on it, I would look into rigging and animating it, as well as combining it with FX in Houdini. This project has helped me gain better understanding of USD, and how it works in the animation and VFX pipeline, and I can't wait to use it in more of my projects. \n\n*Written Oct 21, 2024 by Christopher Hosken*","src/content/posts/jupiter.md","2506c1f13a528866",{"html":167,"metadata":168},"\u003Cp>\u003Cimg src=\"/images/content/jupiter/cover.jpg\" alt=\"References\">\u003C/p>\n\u003Ch2 id=\"introduction\">Introduction\u003C/h2>\n\u003Cp>USD has become the “Buzz Word” for the animation and visual effects industry. And I felt (as an aspiring Generalist TD), that learning how USD works would be crucial for landing a job outside of university.\u003C/p>\n\u003Cp>In an effort to explore the capabilities of USD, I decided to create Jupiter, an American steam train from the late 1800s. I’ve always wanted to model a steam train, and I love the wild-west style of the Jupiter. I spent a total of 2 and half months working on this project, and this is by far my favourite asset I’ve worked on yet.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/jupiter/references.png\" alt=\"References\">\u003C/p>\n\u003Cp>I began the project by looking for references online, I used PureRef to bundle them together so I could easily look at them when needed.\u003C/p>\n\u003Ch2 id=\"modelling\">Modelling\u003C/h2>\n\u003Cp>To explore the benefits of USD’s cross DCC abilities, I decided to model inside of Blender. I’m a huge fan of Blender’s modelling tools, and once I transferred the asset into Maya, I polished the model and created the UVs. This was my “payload” file for my USD asset. It contained all the geometry data, including UVs for shading.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/jupiter/modelling.png\" alt=\"Modelling\">\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/jupiter/uvs.png\" alt=\"UVs\">\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/jupiter/wireframe_turn_for_web.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Cp>Above is the final model of Jupiter. The hardest part of the asset was getting all the intricate details inside the train, as there weren’t many references images of those areas.\u003C/p>\n\u003Ch2 id=\"lookdev\">Lookdev\u003C/h2>\n\u003Cp>Once the payload was finished, I began working on shading and lookdev. I was able to convert Albin Merle’s free Blender lookdev environment into USD, which I could open and use inside of Maya and Houdini.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/jupiter/vibrant_turntable.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Cp>With Substance painter, I working on getting the nice wear and tear of an old train. I was also able to make a shading variant look called “dull”, which looks cleaner and more toy-like.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/jupiter/dull_turntable.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Cp>I did my rendering inside of Maya using Arnold, however the asset also works with Houdini’s Karma.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Overall, I’m very happy with this asset. If I had more time to work on it, I would look into rigging and animating it, as well as combining it with FX in Houdini. This project has helped me gain better understanding of USD, and how it works in the animation and VFX pipeline, and I can’t wait to use it in more of my projects.\u003C/p>\n\u003Cp>\u003Cem>Written Oct 21, 2024 by Christopher Hosken\u003C/em>\u003C/p>",{"headings":169,"imagePaths":176,"frontmatter":177},[170,171,172,175],{"depth":28,"slug":81,"text":82},{"depth":28,"slug":143,"text":144},{"depth":28,"slug":173,"text":174},"lookdev","Lookdev",{"depth":28,"slug":120,"text":121},[],{"title":160,"date":161,"role":16,"excerpt":162,"img":94,"type":72},"jupiter.md","mfs",{"id":179,"data":181,"body":186,"filePath":187,"digest":188,"rendered":189,"legacyId":197},{"title":182,"date":183,"role":133,"img":184,"excerpt":185,"type":19},"Maya Fluid Simulator","2024-05-10","MayaFluidSimulator.PNG","An PIC/FLIP particle simulator in Maya.","![Cover](/images/content/mfs/MayaFluidSimulator.PNG)\n\n## Overview\n\nPart of my Technical Arts Production unit at Bournemouth University required us to make a Maya tool with Python. I decided to challenge myself by looking into fluid simulators. Click to read the full [Project Report](/images/content/mfs/mfs_report.pdf).\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/mfs/MFS_DamBreak.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/mfs/MFS_DefaultDonut.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/mfs/MFS_Honey.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n## Tutorial\n\nAs part of the submission, a tutorial video was made to show how the tool is used.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/mfs/MFS_tutorial.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n*Written May 10, 2024 by Christopher Hosken*","src/content/posts/mfs.md","3e949feb988d85ba",{"html":190,"metadata":191},"\u003Cp>\u003Cimg src=\"/images/content/mfs/MayaFluidSimulator.PNG\" alt=\"Cover\">\u003C/p>\n\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>Part of my Technical Arts Production unit at Bournemouth University required us to make a Maya tool with Python. I decided to challenge myself by looking into fluid simulators. Click to read the full \u003Ca href=\"/images/content/mfs/mfs_report.pdf\">Project Report\u003C/a>.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/mfs/MFS_DamBreak.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/mfs/MFS_DefaultDonut.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/mfs/MFS_Honey.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Ch2 id=\"tutorial\">Tutorial\u003C/h2>\n\u003Cp>As part of the submission, a tutorial video was made to show how the tool is used.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/mfs/MFS_tutorial.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Cp>\u003Cem>Written May 10, 2024 by Christopher Hosken\u003C/em>\u003C/p>",{"headings":192,"imagePaths":195,"frontmatter":196},[193,194],{"depth":28,"slug":29,"text":30},{"depth":28,"slug":35,"text":36},[],{"title":182,"date":183,"role":133,"excerpt":185,"img":184,"type":19},"mfs.md","painter",{"id":198,"data":200,"body":205,"filePath":206,"digest":207,"rendered":208,"legacyId":215},{"title":201,"date":202,"role":133,"img":203,"excerpt":204,"type":19},"PAINter","2024-01-10","desert_wip.png","An MSPaint-like application written in C++ and SDL2.","![PAINter](/images/content/painter/desert_wip.png)\n\n# Overview\n\nFor my Programming Principles class at Bournemouth University, we had to write a C++/SDL2 application. I chose to make an MSPaint-like program. You can see the manual and report below.\n\n![PAINter](/images/content/painter/desert.png)\n\nI found that using an old paint software like this extremely frustrating to use. Hence the name, ***PAIN***ter.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/painter/demo.mp4\" type=\"video/mp4\">\n\u003C/video>\n\nThe project repository can be found at https://github.com/cjhosken/PAINter. PAINter's guide and documentation can be read here.\n\n*Written January 10, 2024 by Christopher Hosken*","src/content/posts/painter.md","a51d49f833355a82",{"html":209,"metadata":210},"\u003Cp>\u003Cimg src=\"/images/content/painter/desert_wip.png\" alt=\"PAINter\">\u003C/p>\n\u003Ch1 id=\"overview\">Overview\u003C/h1>\n\u003Cp>For my Programming Principles class at Bournemouth University, we had to write a C++/SDL2 application. I chose to make an MSPaint-like program. You can see the manual and report below.\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/content/painter/desert.png\" alt=\"PAINter\">\u003C/p>\n\u003Cp>I found that using an old paint software like this extremely frustrating to use. Hence the name, \u003Cem>\u003Cstrong>PAIN\u003C/strong>\u003C/em>ter.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/painter/demo.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Cp>The project repository can be found at \u003Ca href=\"https://github.com/cjhosken/PAINter\">https://github.com/cjhosken/PAINter\u003C/a>. PAINter’s guide and documentation can be read here.\u003C/p>\n\u003Cp>\u003Cem>Written January 10, 2024 by Christopher Hosken\u003C/em>\u003C/p>",{"headings":211,"imagePaths":213,"frontmatter":214},[212],{"depth":104,"slug":29,"text":30},[],{"title":201,"date":202,"role":133,"excerpt":204,"img":203,"type":19},"painter.md","rhino",{"id":216,"data":218,"body":222,"filePath":223,"digest":224,"rendered":225,"legacyId":233},{"title":219,"date":220,"role":16,"img":45,"excerpt":221,"type":72},"Rhino FX","2024-12-24","Experimenting with MPM and Solaris environment layouts.","![Cover](/images/content/rhino/cover.png)\n\n## Introduction\n\nRhino FX was an exploriation into the MPM tools in Houdini, as well as how to build environments using Houdini Solaris. I used Quixel assets for the envionment, and found an animation of a Rhino online. The FX was done by me in Houdini.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/rhino/rhino_comp.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n## Breakdown\n\nA viewport animation of the environment. Mostly made up of Quixel Megascans, I re-purposed an old USD Quixel asset generator to quickly make instancable USD assets. The trees were made using Houdini's tree generation tools. I had enough variants to populate the scene without repetition.\n\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/rhino/viewport_v001.mp4\" type=\"video/mp4\">\n\u003C/video>","src/content/posts/rhino.md","b40cf91aef281b09",{"html":226,"metadata":227},"\u003Cp>\u003Cimg src=\"/images/content/rhino/cover.png\" alt=\"Cover\">\u003C/p>\n\u003Ch2 id=\"introduction\">Introduction\u003C/h2>\n\u003Cp>Rhino FX was an exploriation into the MPM tools in Houdini, as well as how to build environments using Houdini Solaris. I used Quixel assets for the envionment, and found an animation of a Rhino online. The FX was done by me in Houdini.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/rhino/rhino_comp.mp4\" type=\"video/mp4\">\n\u003C/video>\n\u003Ch2 id=\"breakdown\">Breakdown\u003C/h2>\n\u003Cp>A viewport animation of the environment. Mostly made up of Quixel Megascans, I re-purposed an old USD Quixel asset generator to quickly make instancable USD assets. The trees were made using Houdini’s tree generation tools. I had enough variants to populate the scene without repetition.\u003C/p>\n\u003Cvideo controls muted>\n  \u003Csource src=\"/images/content/rhino/viewport_v001.mp4\" type=\"video/mp4\">\n\u003C/video>",{"headings":228,"imagePaths":231,"frontmatter":232},[229,230],{"depth":28,"slug":81,"text":82},{"depth":28,"slug":84,"text":85},[],{"title":219,"date":220,"role":16,"excerpt":221,"img":45,"type":72},"rhino.md"]